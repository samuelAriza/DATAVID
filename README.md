#  DATAVID: Pipeline AutomÃ¡tico de AnÃ¡lisis de Datos COVID-19 Colombia

## Universidad EAFIT - ST0263 TÃ³picos Especiales en TelemÃ¡tica
### Trabajo 3: AutomatizaciÃ³n Big Data en Google Cloud Platform

---

##  Tabla de Contenidos

1. [DescripciÃ³n General](#-descripciÃ³n-general)
2. [Arquitectura del Sistema](#-arquitectura-del-sistema)
3. [Requisitos del Proyecto](#-requisitos-del-proyecto)
4. [TecnologÃ­as Utilizadas](#-tecnologÃ­as-utilizadas)
5. [Estructura del Repositorio](#-estructura-del-repositorio)
6. [Prerequisitos](#-prerequisitos)
7. [InstalaciÃ³n y ConfiguraciÃ³n](#-instalaciÃ³n-y-configuraciÃ³n)
8. [Despliegue del Pipeline](#-despliegue-del-pipeline)
9. [EjecuciÃ³n y Monitoreo](#-ejecuciÃ³n-y-monitoreo)
10. [Dashboard de VisualizaciÃ³n](#-dashboard-de-visualizaciÃ³n)
11. [VerificaciÃ³n de Resultados](#-verificaciÃ³n-de-resultados)
12. [API REST - Endpoints](#-api-rest---endpoints)
13. [AnÃ¡lisis Implementados](#-anÃ¡lisis-implementados)
14. [Modelos de Machine Learning](#-modelos-de-machine-learning)
15. [Troubleshooting](#-troubleshooting)
16. [Costos Estimados](#-costos-estimados)
17. [Autores](#-autores)

---

##  DescripciÃ³n General

Este proyecto implementa un **pipeline completo de Big Data** para el anÃ¡lisis automatizado de datos de COVID-19 en Colombia, cumpliendo con los estÃ¡ndares de producciÃ³n de ingenierÃ­a de datos moderna. El sistema integra mÃºltiples fuentes de datos, realiza procesamiento ETL con Apache Spark, ejecuta anÃ¡lisis descriptivos avanzados y modelos de Machine Learning, todo de forma completamente automatizada.

### CaracterÃ­sticas Principales

-  **Ingesta automÃ¡tica** desde 2+ fuentes heterogÃ©neas (API REST + MySQL)
-  **Procesamiento ETL** con Apache Spark en clusters efÃ­meros
-  **29 anÃ¡lisis epidemiolÃ³gicos** profesionales con DataFrames y SparkSQL
-  **5 modelos de Machine Learning** con SparkML (clasificaciÃ³n + clustering)
-  **Arquitectura de 3 zonas**: Raw â†’ Trusted â†’ Refined
-  **Salida dual**: BigQuery (anÃ¡lisis SQL) + API REST (integraciÃ³n)
-  **100% automatizado**: Sin intervenciÃ³n humana
-  **Clusters efÃ­meros**: Auto-creaciÃ³n y auto-destrucciÃ³n
-  **ProgramaciÃ³n semanal**: Cloud Scheduler (Lunes 4:00 AM)

### Datos Procesados

- **100,000 casos** de COVID-19 en Colombia
- **36 departamentos** con informaciÃ³n demogrÃ¡fica
- **894 municipios** con datos de poblaciÃ³n y altitud
- **20 hospitales** con capacidad e infraestructura
- PerÃ­odo: Mayo 2020 - Enero 2022

---

##  Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INGESTA AUTOMÃTICA                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  API Ministerioâ”‚  â”‚  Cloud SQL     â”‚  â”‚  CSV HistÃ³ricosâ”‚         â”‚
â”‚  â”‚  Salud Colombiaâ”‚  â”‚  MySQL         â”‚  â”‚  (Backup)      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚          â”‚                   â”‚                    â”‚                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                              â”‚                                      â”‚
â”‚                   Cloud Functions (Gen2)                            â”‚
â”‚                   - ingest-covid-data (3600s, 2GB)                  â”‚
â”‚                   - ingest-mysql-data (600s, 512MB)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ZONA RAW (3.74 GB)                           â”‚
â”‚  gs://datavid-raw-zone/                                             â”‚
â”‚  â”œâ”€â”€ api/casos/                  (JSON - 100K registros)            â”‚
â”‚  â”œâ”€â”€ csv/historicos/             (CSV - backup)                     â”‚
â”‚  â””â”€â”€ mysql/                      (JSON - 3 tablas)                  â”‚
â”‚      â”œâ”€â”€ departamentos/          (36 registros)                     â”‚
â”‚      â”œâ”€â”€ municipios/             (894 registros)                    â”‚
â”‚      â””â”€â”€ hospitales/             (20 registros)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROCESAMIENTO ETL (PySpark)                      â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Dataproc Cluster (EfÃ­mero)                     â”‚    â”‚
â”‚  â”‚  - Master: n1-standard-4 (4 vCPU, 15 GB RAM)                â”‚    â”‚
â”‚  â”‚  - Workers: 2x n1-standard-4                                â”‚    â”‚
â”‚  â”‚  - DuraciÃ³n: ~20-25 minutos                                 â”‚    â”‚
â”‚  â”‚  - Auto-creado y auto-destruido por Workflow                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  Script: etl_covid_processing.py (10.14 KiB)                        â”‚
â”‚  - Lectura de mÃºltiples fuentes                                     â”‚
â”‚  - JOIN de COVID + MySQL (poblaciÃ³n, regiÃ³n, densidad)              â”‚
â”‚  - Limpieza y transformaciÃ³n                                        â”‚
â”‚  - Particionamiento por aÃ±o/mes                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ZONA TRUSTED (Particionada)                      â”‚
â”‚  gs://datavid-trusted-zone/covid_processed/                         â”‚
â”‚  â”œâ”€â”€ anio_reporte=2020/                                             â”‚
â”‚  â”‚   â”œâ”€â”€ mes_reporte=5/  (2,704 casos)                              â”‚
â”‚  â”‚   â”œâ”€â”€ mes_reporte=6/  (2,759 casos)                              â”‚
â”‚  â”‚   â”œâ”€â”€ ...                                                        â”‚
â”‚  â”‚   â””â”€â”€ mes_reporte=12/ (16,895 casos)                             â”‚
â”‚  â”œâ”€â”€ anio_reporte=2021/ (10,705 casos)                              â”‚
â”‚  â””â”€â”€ anio_reporte=2022/ (19 casos)                                  â”‚
â”‚                                                                     â”‚
â”‚  Formato: Parquet (snappy compression)                              â”‚
â”‚  Esquema: 25 columnas (COVID + MySQL enriched)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ANALYTICS DESCRIPTIVO      â”‚  â”‚   MACHINE LEARNING              â”‚
â”‚   (PySpark DataFrames + SQL) â”‚  â”‚   (SparkML)                     â”‚
â”‚                              â”‚  â”‚                                 â”‚
â”‚  Script: analytics_          â”‚  â”‚  Script: analytics_ml.py        â”‚
â”‚          descriptive.py      â”‚  â”‚          (18.41 KiB)            â”‚
â”‚          (21.4 KiB)          â”‚  â”‚                                 â”‚
â”‚                              â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚ MODELOS SUPERVISADOS:     â”‚  â”‚
â”‚  â”‚ 10 FUNCIONES ANALYTICS:â”‚  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚  â”‚                        â”‚  â”‚  â”‚  â”‚ 1. Random Forest          â”‚  â”‚
â”‚  â”‚ 1. Temporal            â”‚  â”‚  â”‚  â”‚    - Mortalidad (binary)  â”‚  â”‚
â”‚  â”‚ 2. DemogrÃ¡fico         â”‚  â”‚  â”‚  â”‚    - MÃ©tricas: AUC, F1    â”‚  â”‚
â”‚  â”‚ 3. GeogrÃ¡fico          â”‚  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚  â”‚ 4. Mortalidad          â”‚  â”‚  â”‚  â”‚ 2. Random Forest          â”‚  â”‚
â”‚  â”‚ 5. RecuperaciÃ³n        â”‚  â”‚  â”‚  â”‚    - Severidad (3-class)  â”‚  â”‚
â”‚  â”‚ 6. Hotspots            â”‚  â”‚  â”‚  â”‚    - Leve/Moderado/Grave  â”‚  â”‚
â”‚  â”‚ 7. EpidemiologÃ­a       â”‚  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚  â”‚ 8. Dashboard KPIs      â”‚  â”‚  â”‚  â”‚ 3. Logistic Regression    â”‚  â”‚
â”‚  â”‚ 9. EvoluciÃ³n CFR       â”‚  â”‚  â”‚  â”‚    - HospitalizaciÃ³n      â”‚  â”‚
â”‚  â”‚ 10. Infraestructura    â”‚  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚  â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 4 QUERIES SparkSQL:    â”‚  â”‚  â”‚  â”‚ MODELOS NO SUPERVISADOS:  â”‚  â”‚
â”‚  â”‚                        â”‚  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚  â”‚ 1. Ranking deptos      â”‚  â”‚  â”‚  â”‚ 4. K-Means (k=4)          â”‚  â”‚
â”‚  â”‚ 2. Letalidad evolutiva â”‚  â”‚  â”‚  â”‚    - Clustering deptos    â”‚  â”‚
â”‚  â”‚ 3. Edad-regiÃ³n cross   â”‚  â”‚  â”‚  â”‚    - Features: casos,     â”‚  â”‚
â”‚  â”‚ 4. Casos acumulados    â”‚  â”‚  â”‚  â”‚      letalidad, edad      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚                           â”‚  â”‚
â”‚                              â”‚  â”‚  â”‚ 5. K-Means (k=3)          â”‚  â”‚
â”‚  OUTPUT: 29 datasets         â”‚  â”‚  â”‚    - Clustering poblac.   â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚    - Grupos riesgo        â”‚  â”‚
               â”‚                  â”‚  â”‚                           â”‚  â”‚
               â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
               â”‚                  â”‚                                 â”‚
               â”‚                  â”‚  OUTPUT: 5 modelos + mÃ©tricas   â”‚
               â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ZONA REFINED (Resultados)                      â”‚
â”‚  gs://datavid-refined-zone/                                         â”‚
â”‚  â”œâ”€â”€ analytics/ (29 datasets)                                       â”‚
â”‚  â”‚   â”œâ”€â”€ temporal_mensual.parquet                                   â”‚
â”‚  â”‚   â”œâ”€â”€ demografia_edad.parquet                                    â”‚
â”‚  â”‚   â”œâ”€â”€ geografia_departamentos.parquet                            â”‚
â”‚  â”‚   â”œâ”€â”€ dashboard_kpis.parquet                                     â”‚
â”‚  â”‚   â”œâ”€â”€ ... (25 mÃ¡s)                                               â”‚
â”‚  â”‚                                                                  â”‚
â”‚  â””â”€â”€ ml/ (5 modelos + anÃ¡lisis)                                     â”‚
â”‚      â”œâ”€â”€ predictions_mortality.parquet                              â”‚
â”‚      â”œâ”€â”€ predictions_severity.parquet                               â”‚
â”‚      â”œâ”€â”€ predictions_hospitalization.parquet                        â”‚
â”‚      â”œâ”€â”€ clusters_departamentos.parquet                             â”‚
â”‚      â”œâ”€â”€ clusters_grupos_poblacionales.parquet                      â”‚
â”‚      â””â”€â”€ ml_metrics.parquet                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BIGQUERY        â”‚            â”‚   API REST         â”‚
â”‚   (SQL Analytics) â”‚            â”‚   (IntegraciÃ³n)    â”‚
â”‚                   â”‚            â”‚                    â”‚
â”‚  Dataset:         â”‚            â”‚  Cloud Function:   â”‚
â”‚  covid_analytics  â”‚            â”‚  covid-query-api   â”‚
â”‚                   â”‚            â”‚  (512MB, Node.js)  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚                    â”‚
â”‚  â”‚ 6 Tablas:   â”‚  â”‚            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚             â”‚  â”‚            â”‚  â”‚ 9 Endpoints: â”‚  â”‚
â”‚  â”‚ - dashboard â”‚  â”‚            â”‚  â”‚              â”‚  â”‚
â”‚  â”‚   _kpis     â”‚  â”‚            â”‚  â”‚ GET /        â”‚  â”‚
â”‚  â”‚ - geografia â”‚  â”‚            â”‚  â”‚ GET /kpis    â”‚  â”‚
â”‚  â”‚   _deptos   â”‚  â”‚            â”‚  â”‚ GET /deptos  â”‚  â”‚
â”‚  â”‚ - geografia â”‚  â”‚            â”‚  â”‚ GET /munic   â”‚  â”‚
â”‚  â”‚   _munic    â”‚  â”‚            â”‚  â”‚ GET /region  â”‚  â”‚
â”‚  â”‚ - geografia â”‚  â”‚            â”‚  â”‚ GET /tempor  â”‚  â”‚
â”‚  â”‚   _regiones â”‚  â”‚            â”‚  â”‚ GET /consol  â”‚  â”‚
â”‚  â”‚ - temporal  â”‚  â”‚            â”‚  â”‚ GET /top_mun â”‚  â”‚
â”‚  â”‚   _mensual  â”‚  â”‚            â”‚  â”‚ GET /vista   â”‚  â”‚
â”‚  â”‚ - ranking   â”‚  â”‚            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚   _deptos   â”‚  â”‚            â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  URL PÃºblica:      â”‚
â”‚                   â”‚            â”‚  https://us-       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚  central1-datavid- â”‚
â”‚  â”‚ 2 Vistas:   â”‚  â”‚            â”‚  478812.cloud      â”‚
â”‚  â”‚             â”‚  â”‚            â”‚  functions.net/    â”‚
â”‚  â”‚ - top_      â”‚  â”‚            â”‚  covid-query-api   â”‚
â”‚  â”‚   municipiosâ”‚  â”‚            â”‚                    â”‚
â”‚  â”‚ - vista_    â”‚  â”‚            â”‚  Sin autenticaciÃ³n â”‚
â”‚  â”‚   consolidadaâ”‚ â”‚            â”‚  (pÃºblico)         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AUTOMATIZACIÃ“N COMPLETA                          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Cloud Workflows: covid-pipeline-workflow       â”‚    â”‚
â”‚  â”‚                                                             â”‚    â”‚
â”‚  â”‚  FASE 1: Ingesta COVID (invoke ingest-covid-data)           â”‚    â”‚
â”‚  â”‚  FASE 2: Ingesta MySQL (invoke ingest-mysql-data)           â”‚    â”‚
â”‚  â”‚  FASE 3: Crear Cluster Dataproc (efÃ­mero)                   â”‚    â”‚
â”‚  â”‚  FASE 4: Ejecutar ETL PySpark                               â”‚    â”‚
â”‚  â”‚  FASE 5: Ejecutar Analytics PySpark                         â”‚    â”‚
â”‚  â”‚  FASE 6: Ejecutar ML PySpark                                â”‚    â”‚
â”‚  â”‚  FASE 7: Cargar resultados a BigQuery                       â”‚    â”‚
â”‚  â”‚  FASE 8: Destruir Cluster Dataproc                          â”‚    â”‚
â”‚  â”‚                                                             â”‚    â”‚
â”‚  â”‚  DuraciÃ³n total: ~20-25 minutos                             â”‚    â”‚
â”‚  â”‚  Estado: SUCCEEDED                                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         Cloud Scheduler: run-covid-pipeline                 â”‚    â”‚
â”‚  â”‚                                                             â”‚    â”‚
â”‚  â”‚  Cron: 0 4 * * 1  (Lunes 4:00 AM)                           â”‚    â”‚
â”‚  â”‚  Zona horaria: America/Bogota                               â”‚    â”‚
â”‚  â”‚  Target: Workflow covid-pipeline-workflow                   â”‚    â”‚
â”‚  â”‚  Estado: ENABLED                                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

##  Requisitos del Proyecto

Este proyecto cumple con **todos los requisitos** especificados en el Trabajo 3 de ST0263:

| # | Requisito | ImplementaciÃ³n | Evidencia |
|---|-----------|----------------|-----------|
| **1** | **2+ fuentes de datos heterogÃ©neas** |  API REST Ministerio + Cloud SQL MySQL + CSV histÃ³ricos | Cloud Functions, Raw Zone |
| **2** | **Captura e ingesta automÃ¡tica a buckets S3/GCS** |  Cloud Functions Gen2 + Cloud Scheduler | Raw Zone (3.74 GB) |
| **3** | **ETL automÃ¡tico con Spark en EMR/Dataproc** |  PySpark con JOIN COVID+MySQL, particionamiento | Trusted Zone, Jobs Dataproc |
| **4** | **Analytics con DataFrames Y SparkSQL** |  25 anÃ¡lisis DataFrames + 4 queries SparkSQL = 29 datasets | Refined Zone Analytics |
| **5** | **ML con SparkML (â‰¥2 tÃ©cnicas) - OPCIONAL** |  **BONUS**: 5 modelos (3 supervisados + 2 clustering) | Refined Zone ML |
| **6** | **Resultados via Athena (BigQuery) Y API** |  BigQuery (6 tablas + 2 vistas) + API REST (9 endpoints) | BigQuery Dataset, Cloud Function |
| **7** | **AutomatizaciÃ³n SIN intervenciÃ³n humana** |  Workflow 8 fases + Scheduler semanal + Clusters efÃ­meros | Cloud Workflows, Scheduler |

### Cumplimiento: 100% Requisitos Obligatorios + BONUS ML 

---

## TecnologÃ­as Utilizadas

### Google Cloud Platform (GCP)

| Servicio | Uso | ConfiguraciÃ³n |
|----------|-----|---------------|
| **Cloud Storage** | Almacenamiento de datos (Raw/Trusted/Refined) | 4 buckets, 3.74 GB |
| **Cloud SQL MySQL** | Base de datos relacional (departamentos, municipios) | db-f1-micro, 10 GB |
| **Cloud Functions Gen2** | Ingesta automÃ¡tica + API REST | Python 3.11, Node.js 20 |
| **Dataproc** | Procesamiento Spark (ETL, Analytics, ML) | n1-standard-4, clusters efÃ­meros |
| **BigQuery** | Data Warehouse (consultas SQL) | covid_analytics dataset |
| **Cloud Workflows** | OrquestaciÃ³n del pipeline | 8 fases, 20-25 min |
| **Cloud Scheduler** | ProgramaciÃ³n automÃ¡tica | Cron semanal (Lunes 4AM) |
| **IAM** | GestiÃ³n de permisos | Service Account con roles especÃ­ficos |

### Frameworks y Lenguajes

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Apache Spark** | 3.x | Procesamiento distribuido (ETL, Analytics, ML) |
| **PySpark** | 3.x | API Python para Spark |
| **SparkML** | 3.x | Machine Learning distribuido |
| **Python** | 3.11 | Scripting (ETL, Analytics, Ingesta) |
| **Node.js** | 20 | API REST (Cloud Function) |
| **SQL** | - | Queries BigQuery y SparkSQL |

### LibrerÃ­as Python

```txt
pyspark==3.5.0
google-cloud-storage==2.10.0
google-cloud-bigquery==3.11.0
pandas==2.1.0
numpy==1.25.0
requests==2.31.0
pymysql==1.1.0
```

---

##  Estructura del Repositorio

```
DATAVID/
â”‚
â”œâ”€â”€ README.md                           # Este archivo
â”œâ”€â”€ GUIA_VERIFICACION_COMPLETA.md      # GuÃ­a de verificaciÃ³n con comandos
â”œâ”€â”€ requirements.txt                    # Dependencias Python globales
â”œâ”€â”€ populate_mysql.sql                  # Script SQL para poblar MySQL
â”‚
â”œâ”€â”€ ingest_covid_data/                  # Cloud Function: Ingesta API
â”‚   â”œâ”€â”€ main.py                         # FunciÃ³n principal
â”‚   â””â”€â”€ requirements.txt                # Dependencias especÃ­ficas
â”‚
â”œâ”€â”€ ingest_mysql_data/                  # Cloud Function: Ingesta MySQL
â”‚   â”œâ”€â”€ main.py                         # FunciÃ³n principal
â”‚   â””â”€â”€ requirements.txt                # Dependencias especÃ­ficas
â”‚
â”œâ”€â”€ covid_query_api/                    # Cloud Function: API REST
â”‚   â”œâ”€â”€ index.js                        # Endpoints Node.js
â”‚   â””â”€â”€ package.json                    # Dependencias Node.js
â”‚
â”œâ”€â”€ dashboard_function/                 # Cloud Function: Dashboard Web
â”‚   â”œâ”€â”€ main.py                         # FunciÃ³n dashboard (HTML+Chart.js)
â”‚   â”œâ”€â”€ requirements.txt                # Dependencias Flask
â”‚   â”œâ”€â”€ test_local.py                   # Servidor de prueba local
â”‚   â”œâ”€â”€ .gcloudignore                   # Archivos ignorados al desplegar
â”‚   â””â”€â”€ README.md                       # DocumentaciÃ³n dashboard
â”‚
â”œâ”€â”€ etl_covid_processing/               # Scripts PySpark
â”‚   â”œâ”€â”€ etl_covid_processing.py         # ETL principal (JOIN, limpieza)
â”‚   â””â”€â”€ requirements.txt                # Dependencias PySpark
â”‚
â”œâ”€â”€ analytics_descriptive.py            # Analytics: 29 datasets
â”œâ”€â”€ analytics_ml.py                     # Machine Learning: 5 modelos
â”‚
â”œâ”€â”€ covid_pipeline_workflow.yaml        # DefiniciÃ³n del Workflow
â”‚
â””â”€â”€ scripts/                            # Scripts auxiliares
    â”œâ”€â”€ setup_gcp.sh                    # ConfiguraciÃ³n inicial GCP
    â”œâ”€â”€ deploy_functions.sh             # Despliegue Cloud Functions
    â””â”€â”€ cleanup.sh                      # Limpieza de recursos
```

---

##  Prerequisitos

### 1. Cuenta de Google Cloud Platform

- **Proyecto GCP** con billing habilitado
- **Cuota de vCPUs**: MÃ­nimo 12 vCPUs en us-central1
- **Cuota de storage**: MÃ­nimo 100 GB

### 2. Herramientas Locales

```bash
# Instalar Google Cloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# Inicializar gcloud
gcloud init

# Instalar Python 3.11+
sudo apt-get install python3.11 python3.11-venv

# Instalar Node.js 20+
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs
```

### 3. APIs de GCP a Habilitar

```bash
gcloud services enable compute.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable dataproc.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable workflows.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable sqladmin.googleapis.com
gcloud services enable iam.googleapis.com
```

---

##  InstalaciÃ³n y ConfiguraciÃ³n

### Paso 1: Clonar el Repositorio

```bash
git clone <URL_DEL_REPOSITORIO>
cd DATAVID
```

### Paso 2: Configurar Variables de Entorno

```bash
# Editar y ejecutar
export PROJECT_ID="tu-proyecto-gcp"
export REGION="us-central1"
export ZONE="us-central1-a"

gcloud config set project $PROJECT_ID
gcloud config set compute/region $REGION
gcloud config set compute/zone $ZONE
```

### Paso 3: Crear Service Account

```bash
# Crear service account
gcloud iam service-accounts create covid-pipeline-sa \
  --display-name="COVID Pipeline Service Account"

# Asignar roles
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/dataproc.worker"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/storage.admin"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/bigquery.admin"

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/cloudfunctions.invoker"
```

### Paso 4: Crear Cloud Storage Buckets

```bash
# Bucket para zona RAW
gsutil mb -c STANDARD -l $REGION gs://datavid-raw-zone

# Bucket para zona TRUSTED
gsutil mb -c STANDARD -l $REGION gs://datavid-trusted-zone

# Bucket para zona REFINED
gsutil mb -c STANDARD -l $REGION gs://datavid-refined-zone

# Bucket para scripts PySpark
gsutil mb -c STANDARD -l $REGION gs://datavid-scripts
```

### Paso 5: Crear Cloud SQL MySQL

```bash
# Crear instancia MySQL
gcloud sql instances create covid-mysql-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-f1-micro \
  --region=$REGION \
  --root-password="tu_password_seguro"

# Crear base de datos
gcloud sql databases create covid_data \
  --instance=covid-mysql-instance

# Permitir acceso desde Cloud Functions
gcloud sql instances patch covid-mysql-instance \
  --authorized-networks=0.0.0.0/0
```

### Paso 6: Poblar MySQL con Datos

```bash
# Conectarse a MySQL
gcloud sql connect covid-mysql-instance --user=root

# Ejecutar script SQL
USE covid_data;
SOURCE populate_mysql.sql;
```

---

##  Despliegue del Pipeline

### 1. Subir Scripts PySpark a Cloud Storage

```bash
gsutil cp etl_covid_processing/etl_covid_processing.py gs://datavid-scripts/
gsutil cp analytics_descriptive.py gs://datavid-scripts/
gsutil cp analytics_ml.py gs://datavid-scripts/
```

### 2. Desplegar Cloud Functions

#### a) FunciÃ³n de Ingesta COVID

```bash
cd ingest_covid_data

gcloud functions deploy ingest-covid-data \
  --gen2 \
  --runtime=python311 \
  --region=$REGION \
  --source=. \
  --entry-point=main \
  --trigger-http \
  --allow-unauthenticated \
  --memory=2GB \
  --timeout=3600s \
  --service-account=covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com

cd ..
```

#### b) FunciÃ³n de Ingesta MySQL

```bash
cd ingest_mysql_data

gcloud functions deploy ingest-mysql-data \
  --gen2 \
  --runtime=python311 \
  --region=$REGION \
  --source=. \
  --entry-point=main \
  --trigger-http \
  --allow-unauthenticated \
  --memory=512MB \
  --timeout=600s \
  --set-env-vars MYSQL_CONNECTION_NAME=your-project:us-central1:covid-mysql-instance,MYSQL_USER=root,MYSQL_PASSWORD=tu_password,MYSQL_DATABASE=covid_data \
  --service-account=covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com

cd ..
```

#### c) API REST de Consulta

```bash
cd covid_query_api

gcloud functions deploy covid-query-api \
  --gen2 \
  --runtime=nodejs20 \
  --region=$REGION \
  --source=. \
  --entry-point=app \
  --trigger-http \
  --allow-unauthenticated \
  --memory=512MB \
  --timeout=60s \
  --set-env-vars PROJECT_ID=$PROJECT_ID,DATASET_ID=covid_analytics \
  --service-account=covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com

cd ..
```

### 3. Desplegar Cloud Workflow

```bash
gcloud workflows deploy covid-pipeline-workflow \
  --source=covid_pipeline_workflow.yaml \
  --location=$REGION \
  --service-account=covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com
```

### 4. Configurar Cloud Scheduler

```bash
gcloud scheduler jobs create http run-covid-pipeline \
  --location=$REGION \
  --schedule="0 4 * * 1" \
  --time-zone="America/Bogota" \
  --uri="https://workflowexecutions.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/workflows/covid-pipeline-workflow/executions" \
  --http-method=POST \
  --oauth-service-account-email=covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com
```

---

##  EjecuciÃ³n y Monitoreo

### EjecuciÃ³n Manual del Workflow

```bash
# Ejecutar workflow
gcloud workflows run covid-pipeline-workflow --location=$REGION

# El comando retornarÃ¡ un ID de ejecuciÃ³n, por ejemplo:
# Execution ID: 982a393c-ae3c-4dc5-81c3-6299048050c8
```

### Monitoreo en Tiempo Real

```bash
# Guardar el ID de ejecuciÃ³n
export EXEC_ID="TU_EXECUTION_ID"

# Ver estado actual
gcloud workflows executions describe $EXEC_ID \
  --workflow=covid-pipeline-workflow \
  --location=$REGION \
  --format="value(state)"

# Monitorear cada 30 segundos (tarda ~20-25 minutos)
watch -n 30 "gcloud workflows executions describe $EXEC_ID \
  --workflow=covid-pipeline-workflow \
  --location=$REGION \
  --format='value(state)'"
```

### Ver Jobs de Dataproc

```bash
# Jobs ETL
gcloud dataproc jobs list --region=$REGION \
  --filter="yarnApplications.name:COVID-ETL-Processing" \
  --limit=5

# Jobs Analytics
gcloud dataproc jobs list --region=$REGION \
  --filter="yarnApplications.name:COVID-Analytics-Descriptive" \
  --limit=5

# Jobs ML
gcloud dataproc jobs list --region=$REGION \
  --filter="yarnApplications.name:COVID-Analytics-ML" \
  --limit=5
```

### Ver Logs

```bash
# Logs del workflow
gcloud logging read "resource.type=workflows.googleapis.com/Workflow" \
  --limit=50 \
  --format="table(timestamp,jsonPayload.message)"

# Logs de Cloud Functions
gcloud functions logs read ingest-covid-data --region=$REGION --limit=20
```

---

##  Dashboard de VisualizaciÃ³n

### DescripciÃ³n

Dashboard web profesional desplegado como **Cloud Function HTTP** que consume la API `covid-query-api` y presenta visualizaciones interactivas con **Chart.js** directamente integrado con **BigQuery**.

** URL del Dashboard**: 
```
https://us-central1-datavid-478812.cloudfunctions.net/covid-dashboard
```

**CaracterÃ­sticas**:
-  **6 KPIs principales**: Casos totales, fallecidos, recuperados, letalidad promedio, edad promedio, departamentos analizados
-  **Serie temporal mensual**: GrÃ¡fico de lÃ­neas con evoluciÃ³n de casos y fallecidos por mes
-  **Top 10 departamentos por letalidad**: Barras horizontales con las tasas de letalidad mÃ¡s altas
-  **DistribuciÃ³n por regiÃ³n**: GrÃ¡fico doughnut con casos por regiÃ³n geogrÃ¡fica
-  **Casos por 100k habitantes**: Barras verticales con incidencia normalizada por poblaciÃ³n
-  **Tabla BigQuery consolidada**: 36 departamentos con mÃ©tricas epidemiolÃ³gicas completas (regiÃ³n, poblaciÃ³n, casos/100k)
-  **DiseÃ±o responsivo profesional**: Adaptable a mÃ³vil, tablet y desktop con gradientes modernos
-  **Carga dinÃ¡mica hÃ­brida**: API REST + BigQuery en tiempo real
-  **Pipeline completo verificado**: Datos actualizados tras ejecuciÃ³n del workflow automÃ¡tico

### Despliegue del Dashboard

```bash
# Desde el directorio dashboard_function/
cd dashboard_function

# Desplegar Cloud Function con configuraciÃ³n actualizada
gcloud functions deploy covid-dashboard \
  --gen2 \
  --runtime=python311 \
  --region=us-central1 \
  --source=. \
  --entry-point=dashboard \
  --trigger-http \
  --allow-unauthenticated \
  --memory=1GB \
  --timeout=120s \
  --set-env-vars API_URL=https://covid-query-api-7i72qatckq-uc.a.run.app,PROJECT_ID=datavid-478812

# Obtener URL del dashboard
DASHBOARD_URL=$(gcloud functions describe covid-dashboard \
  --region=us-central1 \
  --gen2 \
  --format="value(serviceConfig.uri)")

echo " Dashboard URL: $DASHBOARD_URL"
```

**Salida esperada**:
```
Deploying function (may take a while - up to 2 minutes)...done.
serviceConfig:
  uri: https://us-central1-datavid-478812.cloudfunctions.net/covid-dashboard
  availableMemory: 1Gi
  timeout: 120s
status: ACTIVE
environment: GEN_2
```

**Dashboard desplegado en**: `https://us-central1-datavid-478812.cloudfunctions.net/covid-dashboard`

### VerificaciÃ³n del Dashboard

#### 1. Verificar despliegue exitoso

```bash
gcloud functions describe covid-dashboard \
  --region=us-central1 \
  --gen2 \
  --format="table(name,state,serviceConfig.uri,serviceConfig.availableMemory)"
```

**Esperado**:
```
NAME             STATE   URI                                                           MEMORY
covid-dashboard  ACTIVE  https://us-central1-datavid-478812.cloudfunctions.net/...    1Gi
```

#### 2. Probar respuesta HTTP

```bash
# Verificar que retorna HTML
curl -s "$DASHBOARD_URL" | head -20

# Debe mostrar:
# <!DOCTYPE html>
# <html lang="es">
# <head>
#     <meta charset="UTF-8">
#     <title>COVID-19 Colombia - Dashboard AnalÃ­tico</title>
#     ...
```

#### 3. Abrir en navegador

```bash
# Linux
xdg-open "$DASHBOARD_URL"

# macOS
open "$DASHBOARD_URL"

# Windows (desde Git Bash)
start "$DASHBOARD_URL"
```

**O copiar la URL y abrirla manualmente en Chrome/Firefox.**

### Elementos del Dashboard

Al abrir el dashboard en el navegador, se visualizan:

1. **Header profesional**:
   - TÃ­tulo: "ğŸ“Š COVID-19 Colombia"
   - SubtÃ­tulo: "Dashboard AnalÃ­tico Profesional - Big Data Pipeline"
   - Badges de tecnologÃ­as: Apache Spark | GCP Dataproc | BigQuery | Cloud Workflows | Cloud Functions
   - DiseÃ±o con gradiente morado/azul

2. **Grid de KPIs** (6 tarjetas con animaciÃ³n hover):
   - Total de casos (formato con separadores de miles)
   - Total de fallecidos
   - Total de recuperados
   - Letalidad promedio (%)
   - Edad promedio (aÃ±os)
   - Departamentos analizados

3. **GrÃ¡ficos interactivos Chart.js**:
   - **Serie temporal mensual**: LÃ­neas superpuestas (casos en azul, fallecidos en rojo)
     - Eje X: Meses (formato YYYY-MM)
     - Eje Y: Cantidad de casos
     - Tooltips dinÃ¡micos al pasar el mouse
     - Leyenda interactiva para ocultar/mostrar series
   
   - **Top 10 departamentos por letalidad**: Barras horizontales rojas ordenadas descendentemente
     - Tooltips con valores exactos en porcentaje
     - Departamentos ordenados por tasa de letalidad
   
   - **DistribuciÃ³n por regiÃ³n**: GrÃ¡fico doughnut (dona)
     - 4 regiones: Andina, Caribe, PacÃ­fica, OrinoquÃ­a/AmazonÃ­a
     - Paleta de colores profesional
     - Tooltips con cantidad de casos y porcentaje
   
   - **Casos por 100k habitantes (Top 10)**: Barras verticales moradas
     - NormalizaciÃ³n por poblaciÃ³n (casos per cÃ¡pita)
     - Permite comparar incidencia real entre departamentos de diferente tamaÃ±o

4. **Tabla BigQuery Consolidada**:
   - **36 departamentos** con datos completos tras actualizaciÃ³n de MySQL
   - Columnas: Ranking | Departamento | RegiÃ³n | PoblaciÃ³n | Casos | Fallecidos | Letalidad | Casos/100k
   - IntegraciÃ³n directa con BigQuery (tabla `geografia_departamentos`)
   - Datos enriquecidos con JOIN COVID + MySQL (regiÃ³n, poblaciÃ³n, casos normalizados)
   - Formato numÃ©rico: separadores de miles, decimales controlados
   - Ordenamiento: Letalidad descendente

5. **Footer**:
   - CrÃ©ditos del proyecto: "ğŸ“ Trabajo 3 - AutomatizaciÃ³n Big Data"
   - InstituciÃ³n: Universidad EAFIT | ST0263: TÃ³picos Especiales en TelemÃ¡tica | 2025-2
   - Fuente de datos: Ministerio de Salud Colombia
   - Infraestructura: Apache Spark (GCP Dataproc) + Cloud Workflows
   - Arquitectura: Google Cloud Platform

### Arquitectura del Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Usuario (Navegador Web)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ HTTPS GET
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Function: covid-dashboard                   â”‚
â”‚  - Runtime: Python 3.11                            â”‚
â”‚  - Memoria: 512MB                                  â”‚
â”‚  - Timeout: 60s                                    â”‚
â”‚  - Endpoint: /                                     â”‚
â”‚  - Retorna: HTML + CSS + JavaScript                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  HTML Template        â”‚
        â”‚  - Chart.js (CDN)     â”‚
        â”‚  - Fetch API          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ HTTPS GET
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Function: covid-query-api                   â”‚
â”‚  - Endpoints: /temporal, /departamentos            â”‚
â”‚  - Retorna: JSON (datos analÃ­ticos)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Consulta
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BigQuery: covid_analytics                         â”‚
â”‚  - Tablas: temporal_mensual, geografia_departam... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Troubleshooting Dashboard

#### Error: "Variable de entorno API_URL no configurada"

**Causa**: No se pasÃ³ la variable al desplegar.

**SoluciÃ³n**:
```bash
gcloud functions deploy covid-dashboard \
  --gen2 \
  --runtime=python311 \
  --region=us-central1 \
  --source=. \
  --entry-point=dashboard \
  --trigger-http \
  --allow-unauthenticated \
  --set-env-vars API_URL=https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api
```

#### Error: "Error al cargar datos" en el navegador

**Causa**: La API no responde o URL incorrecta.

**VerificaciÃ³n**:
```bash
API_URL="https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api"
curl -s "$API_URL/temporal?limit=3" | jq '.'
```

Si no responde, verificar que la API estÃ© activa:
```bash
gcloud functions describe covid-query-api --region=us-central1 --gen2
```

#### GrÃ¡ficos no cargan

**Causa posible**: Problema con CDN de Chart.js.

**SoluciÃ³n**: Revisar consola del navegador (F12 â†’ Console) para errores de red. Verificar conectividad al CDN `cdn.jsdelivr.net`.

---

##  VerificaciÃ³n de Resultados

### 1. Verificar Zona Raw

```bash
gsutil ls gs://datavid-raw-zone/api/casos/ | tail -5
gsutil ls gs://datavid-raw-zone/mysql/departamentos/ | tail -1
gsutil du -sh gs://datavid-raw-zone/
```

**Esperado**: 
- Archivos JSON con timestamp
- TamaÃ±o total: ~3.74 GB

### 2. Verificar Zona Trusted

```bash
gsutil ls gs://datavid-trusted-zone/covid_processed/ | grep "anio_reporte="
gsutil ls gs://datavid-trusted-zone/covid_processed/anio_reporte=2020/
```

**Esperado**:
- 3 particiones por aÃ±o (2020, 2021, 2022)
- 8 particiones por mes en 2020
- Archivos Parquet (snappy compressed)

### 3. Verificar Zona Refined - Analytics

```bash
gsutil ls gs://datavid-refined-zone/analytics/ | grep "parquet/$" | wc -l
gsutil ls gs://datavid-refined-zone/analytics/ | head -15
```

**Esperado**:
- **29 datasets** analytics
- Archivos en formato Parquet

### 4. Verificar Zona Refined - ML

```bash
gsutil ls gs://datavid-refined-zone/ml/ | grep -E "predictions|clusters|metrics"
```

**Esperado**:
- 3 archivos de predicciones (mortality, severity, hospitalization)
- 2 archivos de clustering (departamentos, poblacional)
- 1 archivo de mÃ©tricas

### 5. Verificar BigQuery

```bash
bq ls datavid-478812:covid_analytics

bq query --use_legacy_sql=false \
"SELECT total_casos, total_fallecidos, tasa_letalidad_general 
FROM \`${PROJECT_ID}.covid_analytics.dashboard_kpis\`"
```

**Esperado**:
- 6 tablas + 2 vistas
- Dashboard KPIs con 100,000 casos

### 6. Verificar API REST

```bash
export API_URL="https://us-central1-${PROJECT_ID}.cloudfunctions.net/covid-query-api"

# Endpoints disponibles
curl -s $API_URL/ | jq '.endpoints'

# KPIs nacionales
curl -s $API_URL/kpis | jq '.'

# Top 5 departamentos
curl -s "$API_URL/departamentos?limit=5" | jq '.[] | {departamento: .nombre_departamento, casos: .total_casos}'
```

**Esperado**:
- 9 endpoints funcionando
- Respuestas en formato JSON
- Sin requerir autenticaciÃ³n

---

## API REST - Endpoints

### Base URL

```
https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api
```

### Endpoints Disponibles

| MÃ©todo | Endpoint | DescripciÃ³n | ParÃ¡metros |
|--------|----------|-------------|------------|
| `GET` | `/` | Lista de endpoints disponibles | - |
| `GET` | `/kpis` | KPIs nacionales (casos, fallecidos, CFR) | - |
| `GET` | `/departamentos` | Ranking departamentos por casos | `?limit=N` |
| `GET` | `/municipios` | Top municipios afectados | `?limit=N` |
| `GET` | `/regiones` | EstadÃ­sticas por regiÃ³n geogrÃ¡fica | - |
| `GET` | `/temporal` | EvoluciÃ³n mensual de casos | `?limit=N` |
| `GET` | `/consolidado` | Vista consolidada completa | `?limit=N` |
| `GET` | `/top-municipios` | Top municipios (vista BigQuery) | - |
| `GET` | `/vista-consolidada` | Vista consolidada (vista BigQuery) | - |

### Ejemplos de Uso

```bash
# KPIs nacionales
curl "https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api/kpis"

# Response:
{
  "total_casos": 100000,
  "total_fallecidos": 2808,
  "total_recuperados": 96742,
  "tasa_letalidad_general": 2.808,
  "departamentos_afectados": 36,
  "municipios_afectados": 894,
  "edad_promedio": 40.2
}

# Top 10 departamentos
curl "https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api/departamentos?limit=10"

# EvoluciÃ³n temporal (Ãºltimos 6 meses)
curl "https://us-central1-datavid-478812.cloudfunctions.net/covid-query-api/temporal?limit=6"
```

### Dashboard Web Interactivo

AdemÃ¡s de la API REST, el proyecto incluye un **dashboard web profesional** desplegado como Cloud Function:

**URL del Dashboard**:
```
https://us-central1-datavid-478812.cloudfunctions.net/covid-dashboard
```

**CaracterÃ­sticas**:
-  4 KPIs principales (casos, fallecidos, letalidad, departamentos)
-  GrÃ¡fico de serie temporal mensual (Chart.js)
-  Top 10 departamentos por letalidad (barras horizontales)
-  Tabla completa con ranking de departamentos
-  DiseÃ±o responsivo (mÃ³vil, tablet, desktop)
-  ActualizaciÃ³n en tiempo real (consume API REST)

Ver [secciÃ³n completa del Dashboard](#-dashboard-de-visualizaciÃ³n) para instrucciones de despliegue.

---

##  AnÃ¡lisis Implementados

### AnÃ¡lisis con DataFrames (25 datasets)

#### 1. AnÃ¡lisis Temporal (2)
- `temporal_mensual.parquet`: AgregaciÃ³n mensual de casos, fallecidos, recuperados
- `cfr_temporal_departamento.parquet`: EvoluciÃ³n de tasa de letalidad por departamento

#### 2. AnÃ¡lisis DemogrÃ¡fico (4)
- `demografia_edad.parquet`: DistribuciÃ³n por grupos etarios
- `demografia_sexo.parquet`: DistribuciÃ³n por sexo
- `demografia_edad_sexo.parquet`: DistribuciÃ³n cruzada edad-sexo
- `estado_casos.parquet`: DistribuciÃ³n por estado (recuperado, fallecido, activo)

#### 3. AnÃ¡lisis GeogrÃ¡fico (3)
- `geografia_departamentos.parquet`: EstadÃ­sticas por departamento + poblaciÃ³n
- `geografia_municipios_top50.parquet`: Top 50 municipios mÃ¡s afectados
- `geografia_regiones.parquet`: AgregaciÃ³n por regiÃ³n geogrÃ¡fica

#### 4. AnÃ¡lisis de Mortalidad (5)
- `mortalidad_edad_sexo.parquet`: Letalidad por edad y sexo
- `mortalidad_departamento_mes.parquet`: EvoluciÃ³n mensual por departamento
- `tiempo_diagnostico_recuperacion.parquet`: Tiempo promedio hasta recuperaciÃ³n
- `tiempo_diagnostico_muerte.parquet`: Tiempo promedio hasta fallecimiento
- `mortalidad_por_tipo_recuperacion.parquet`: Letalidad segÃºn tipo de recuperaciÃ³n

#### 5. AnÃ¡lisis de RecuperaciÃ³n (3)
- `recuperacion_por_tipo.parquet`: DistribuciÃ³n de tipos de recuperaciÃ³n
- `tiempo_recuperacion_edad.parquet`: Tiempo de recuperaciÃ³n por edad
- `estado_salud_actual.parquet`: Estado de salud reportado

#### 6. Hotspots y CrÃ­ticos (2)
- `municipios_criticos.parquet`: Municipios con alta incidencia
- `departamentos_criticos.parquet`: Departamentos con alto riesgo

#### 7. Dashboard KPIs (3)
- `dashboard_kpis.parquet`: KPIs nacionales consolidados
- `dashboard_evolucion.parquet`: EvoluciÃ³n temporal para dashboard
- `dashboard_top_departamentos.parquet`: Ranking para visualizaciÃ³n

#### 8. Indicadores EpidemiolÃ³gicos (3)
- `indicadores_epidemiologicos.parquet`: R0, tasa de ataque, CFR evolutivo
- `indice_gravedad.parquet`: Ãndice de gravedad por regiÃ³n
- `infraestructura_salud.parquet`: Capacidad hospitalaria vs casos

### AnÃ¡lisis con SparkSQL (4 queries)

#### 1. Ranking Departamentos
```sql
SELECT 
  nombre_departamento,
  total_casos,
  fallecidos,
  RANK() OVER (ORDER BY total_casos DESC) as ranking
FROM casos_departamentos
```

#### 2. Letalidad Evolutiva
```sql
SELECT 
  mes,
  (SUM(fallecidos) / SUM(casos)) * 100 as tasa_letalidad
FROM casos_temporales
GROUP BY mes
ORDER BY mes
```

#### 3. AnÃ¡lisis Edad-RegiÃ³n
```sql
SELECT 
  region,
  grupo_edad,
  COUNT(*) as casos,
  AVG(edad) as edad_promedio
FROM casos_enriquecidos
GROUP BY region, grupo_edad
```

#### 4. Casos Acumulados
```sql
SELECT 
  fecha,
  SUM(casos) OVER (ORDER BY fecha) as acumulado
FROM casos_diarios
```

---

##  Modelos de Machine Learning

### Modelos Supervisados (3)

#### 1. Random Forest - PredicciÃ³n de Mortalidad (Binary Classification)

**Objetivo**: Predecir si un paciente fallecerÃ¡ dado su perfil

**Features**:
- `edad_anios` (numÃ©rico)
- `sexo_idx` (0=F, 1=M)
- `departamento_idx` (StringIndexer)
- `estado_idx` (StringIndexer)
- `recuperacion_idx` (StringIndexer)
- `ubicacion_idx` (StringIndexer)

**Target**: `fallecido` (0=No, 1=SÃ­)

**MÃ©tricas**:
- AUC-ROC: 1.0
- Accuracy: 100%
- F1-Score: 1.0

**Feature Importance**:
1. `ubicacion_idx`: 95%
2. `edad_anios`: 3%
3. `sexo_idx`: 1%
4. Otros: <1%

**Output**: `predictions_mortality.parquet`

---

#### 2. Random Forest - ClasificaciÃ³n de Severidad (Multiclass)

**Objetivo**: Clasificar casos en Leve, Moderado o Grave

**Features**: Mismas que modelo 1

**Target**: `severidad` (3 clases)
- 0 = Leve
- 1 = Moderado
- 2 = Grave

**MÃ©tricas**:
- Accuracy: 100%
- F1-Score: 1.0
- Precision: 1.0
- Recall: 1.0

**Output**: `predictions_severity.parquet`

---

#### 3. Logistic Regression - Riesgo de HospitalizaciÃ³n

**Objetivo**: Predecir si un paciente requerirÃ¡ hospitalizaciÃ³n

**Features**: Mismas que modelo 1

**Target**: `requiere_hospitalizacion` (0=No, 1=SÃ­)

**MÃ©tricas**:
- AUC-ROC: 0.0 (dataset con clase Ãºnica en test)
- Accuracy: 100%

**Nota**: Modelo incluye manejo de error para datasets con clase Ãºnica

**Output**: `predictions_hospitalization.parquet`

---

### Modelos No Supervisados (2)

#### 4. K-Means - Clustering de Departamentos (k=4)

**Objetivo**: Segmentar departamentos por perfil epidemiolÃ³gico

**Features**:
- `total_casos`
- `edad_promedio`
- `tasa_letalidad`
- `tasa_recuperacion`
- `poblacion`
- `densidad`
- `incidencia_100k`

**Clusters**:
- **Cluster 0**: Departamentos con baja densidad, letalidad moderada
- **Cluster 1**: BOGOTÃ (outlier - alta densidad, muchos casos)
- **Cluster 2**: Departamentos con baja incidencia
- **Cluster 3**: Departamentos medianos, letalidad moderada-alta

**Output**: `clusters_departamentos.parquet`

**AnÃ¡lisis**: `clusters_analisis_departamentos.parquet`

---

#### 5. K-Means - Clustering de Grupos Poblacionales (k=3)

**Objetivo**: Identificar grupos de riesgo por edad/sexo

**Features**:
- `edad_anios`
- `sexo_idx`
- `tasa_mortalidad`
- `tiempo_recuperacion`

**Clusters**:
- **Cluster 0**: PoblaciÃ³n joven (20-40 aÃ±os), baja mortalidad
- **Cluster 1**: PoblaciÃ³n adulta (40-60 aÃ±os), mortalidad moderada
- **Cluster 2**: PoblaciÃ³n mayor (60+ aÃ±os), alta mortalidad

**Output**: `clusters_grupos_poblacionales.parquet`

**AnÃ¡lisis**: `clusters_analisis_poblacional.parquet`

---

### Archivo de MÃ©tricas Consolidadas

**Archivo**: `ml_metrics.parquet`

**Contenido**:
```csv
mortality_accuracy,mortality_auc,mortality_f1,
severity_accuracy,severity_precision,severity_recall,severity_f1,
hospitalization_accuracy,hospitalization_auc
```

---

##  Troubleshooting

### Problema 1: Workflow Falla en CreaciÃ³n de Cluster

**Error**: `Insufficient 'CPUS' quota in region us-central1`

**SoluciÃ³n**:
```bash
# Solicitar aumento de cuota en GCP Console:
# IAM & Admin > Quotas > Compute Engine API > CPUs (us-central1)
# Solicitar: 16 CPUs mÃ­nimo
```

---

### Problema 2: Cloud Function Timeout

**Error**: `Function execution took 541000 ms, finished with status: timeout`

**SoluciÃ³n**:
```bash
# Aumentar timeout a 3600s (1 hora)
gcloud functions deploy ingest-covid-data \
  --timeout=3600s \
  --memory=2GB
```

---

### Problema 3: BigQuery Access Denied

**Error**: `Access Denied: BigQuery BigQuery: Permission bigquery.tables.create denied`

**SoluciÃ³n**:
```bash
# Asignar rol BigQuery Admin al service account
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:covid-pipeline-sa@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/bigquery.admin"
```

---

### Problema 4: Dataproc Job ERROR

**Error**: `Job failed with error: File not found gs://datavid-scripts/etl_covid_processing.py`

**SoluciÃ³n**:
```bash
# Verificar que scripts estÃ©n subidos
gsutil ls gs://datavid-scripts/

# Subir scripts
gsutil cp etl_covid_processing/etl_covid_processing.py gs://datavid-scripts/
gsutil cp analytics_descriptive.py gs://datavid-scripts/
gsutil cp analytics_ml.py gs://datavid-scripts/
```

---

### Problema 5: MySQL Connection Refused

**Error**: `ERROR 2003 (HY000): Can't connect to MySQL server`

**SoluciÃ³n**:
```bash
# Verificar que Cloud SQL estÃ© running
gcloud sql instances describe covid-mysql-instance

# Permitir conexiones desde Cloud Functions
gcloud sql instances patch covid-mysql-instance \
  --authorized-networks=0.0.0.0/0

# O usar Cloud SQL Proxy
```

---

##  Costos Estimados

### Costos Mensuales Estimados (EjecuciÃ³n Semanal)

| Servicio | ConfiguraciÃ³n | Costo/Mes (USD) |
|----------|---------------|-----------------|
| **Cloud Storage** | 3.74 GB Standard + 2 GB Refined | $0.10 |
| **Cloud SQL MySQL** | db-f1-micro, 10 GB | $7.50 |
| **Cloud Functions** | 4 ejecuciones/mes, 2GB RAM | $2.00 |
| **Dataproc** | 4 clusters efÃ­meros/mes, 20 min c/u | $4.00 |
| **BigQuery** | Queries + storage (1 GB) | $0.50 |
| **Cloud Workflows** | 4 ejecuciones/mes | $0.10 |
| **Cloud Scheduler** | 1 job | $0.10 |
| **Networking** | Egress + API calls | $0.50 |
| **TOTAL** | - | **~$15/mes** |

### Optimizaciones de Costo

1. **Clusters EfÃ­meros**: Solo pagan mientras corren (~20 min)
2. **Preemptible Workers**: Reducir costo Dataproc en 80%
3. **Bucket Nearline**: Para zona Raw (datos histÃ³ricos)
4. **Cloud SQL Pause**: Detener MySQL cuando no se use
5. **BigQuery Flat-Rate**: Para consultas frecuentes

---

## Autores

**Samuel AndrÃ©s Ariza GÃ³mez**  

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/samargo) [![GitHub](https://img.shields.io/badge/GitHub-000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/samuelAriza)

---

##  Licencia

Este proyecto es parte de un trabajo acadÃ©mico para la Universidad EAFIT.

---

##  Enlaces Ãštiles

- [DocumentaciÃ³n GCP Dataproc](https://cloud.google.com/dataproc/docs)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [BigQuery SQL Reference](https://cloud.google.com/bigquery/docs/reference/standard-sql)
- [Cloud Workflows Syntax](https://cloud.google.com/workflows/docs/reference/syntax)
- [SparkML Guide](https://spark.apache.org/docs/latest/ml-guide.html)

---

##  Soporte

Para preguntas o problemas:

1. **Logs**: `gcloud logging read` para cada servicio
2. **GCP Console**: Verificar estado de recursos
3. **Stack Overflow**: Tag `google-cloud-platform` + `pyspark`

---

**Proyecto desarrollado como parte del curso ST0263 - Universidad EAFIT**

**Pipeline Big Data Automatizado - COVID-19 Colombia**

**Noviembre 2024**
